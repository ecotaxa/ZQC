import math
import labels
import time
import numpy as np
import re
import logging
from datetime import datetime

now= datetime.now()
logging.basicConfig(filename="logs/"+str(now.year)+"-"+str(now.month)+".log", level = logging.INFO, format="%(asctime)s | %(levelname)s | %(threadName)s |%(message)s")

#### TOOLS

def is_float(value):
    """Return true if value is a float else return false"""
    try:
        float(value)
        return True
    except BaseException:
        return False

def is_int(value):
    """Return true if value is an int else return false"""
    try:
        int(value)
        return True
    except BaseException:
        return False
def is_not_int(value):
    return not(is_int(value))

def get_frac_id(str) : 
    '''Return the frac id from the giver str if this one is in the non exaustive list of frac type [d1, d2, d3, dN, tot, plankton] '''
    frac_types = ["(_d){1}([1-9])+(_){1}", "(_tot_){1}", "(_plankton_){1}"]

    for element in frac_types:
        m = re.search(element, str)
        if m:
            return m.group(0)
    return "frac_type_not_handled"

def is_power_of_two(n):
    return (n & (n-1) == 0) and n != 0

def independent_concat(df1_max, col1, df2_min, col2): 
    ''' set the df2_min size of a collumn in order to integrate it into de bigger df'''
    tmp_min = []

    for e in df2_min[col2].values : 
        tmp_min.append(e)
    for i in range(len(df2_min[col2].values),len(df1_max[col1].values)) : 
        tmp_min.append(" ")
    
    df1_max[col2]=tmp_min
    return df1_max

#### CALLBACKS

#Returns information by samples about the absence of implementation of this QC
def noCb(_id, _mode, local_data):
    """This feature will be available in a future release of the QC application."""
    logging.info("{} : {} : WIP callback not implemented yet".format(_id, _mode))
    result = local_data.get("dataframe")[['scan_id']].drop_duplicates()
    result[_id] = "Not impl"#labels.errors["global.qc_not_implemented"]
    result.rename(columns={'scan_id': 'List scan ID'}, inplace=True)
    return result

### PROCESS
def check_frame_type(_id, _mode, local_data):
    """Displays information about the size of the frame used for scanning: "large" or "narrow".
       In the "Frame type" column of the report table:
        - "large" : if the process_img_background_img of ecotaxa.tsv tables contains "large" and the .ini file name in Zooscan_config folder contains "large"
        - "narrow" : if the process_img_background_img of ecotaxa.tsv tables contains "narrow" and the .ini file name in Zooscan_config folder contains "narrow"
        - "#MISSING ecotaxa table" : if no ecotaxa_scanID.tsv table
        - "#MISSING column" : if process_img_background_img column is missing from the ecotaxa.tsv table
        - "#Frame NOT OK" : if any other issue
    """
    start_time = time.time()

    # Get only usefull columns
    result = local_data.get("dataframe")[['scan_id', 'process_img_background_img']].drop_duplicates()

    # Get only usefull file name : .ini in Zooscan_config folder
    fs = local_data.get("fs")
    dataToTest = fs.loc[([True if "/Zooscan_config/" in i else False for i in fs['path'].values])
                        & (fs['extension'].values == "ini"), ["name", "extension"]].name.values
    ini_file_name = dataToTest[0] if len(dataToTest)==1 else ""

    # Replace by large or narrow or associated error code
    result.process_img_background_img = result.process_img_background_img.map(lambda x: "large" if ("large" in x) and ("large" in ini_file_name)
                                                                              else "narrow" if ("narrow" in x) and ("narrow" in ini_file_name)
                                                                              else x if labels.errors["global.missing_ecotaxa_table"] == x
                                                                              else x if x==labels.errors["global.missing_column"]
                                                                              else labels.errors["process.frame_type.not_ok"])                  

    # Keep only one line by couples : id / frame type
    result = result.drop_duplicates()

    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', 'process_img_background_img': 'Frame type'}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_frame_type".format((time.time() - start_time), _id, _mode))
    return result

def check_raw_files(_id, _mode, local_data):
    """Checks the file system structure generated by zooprocess for the process and scan steps.
    In the _raw directory of Zooscan_scan should be present :
        - 1 file scanID_log.txt
        - 1 file scanID_meta.txt
        - 1 image sampleID_fracID_raw_1.tif and/or 1 file sampleID_fracID_raw_1.zip
    In the collumn "RAW files" of the report table:
        - "#MISSING FILE" : one of the above listed files is missing
        - "#DUPLICATE FILE" : one of the above listed files is duplicated
        - "#bug RENAME ZIP FILE" : if a zip is present, but the inside name isn't the same as the zip name
        - "Files OK" :  Everything is OK, the number of files is as expected
    """
    start_time = time.time()

    # Get only usefull column size : where path contains /_raw/
    fs = local_data.get("fs")
    dataToTest = fs.loc[([True if "/_raw/" in i else False for i in fs['path'].values]), ["name", "extension", "inside_name"]]
    result = local_data.get("dataframe")[['scan_id']].drop_duplicates()
    result["raw_files"] = ""

    # Extract scan ids
    ids= [id[:-2] if id.endswith('_1') else id for id in result.scan_id.unique()]

    # foreatch scan id
    for id in ids:
        datascanId = dataToTest.loc[([True if id in i else False for i in dataToTest['name'].values]), ["name", "extension", "inside_name"]]

        # count of scanID_log.txt should be 1
        count_log = datascanId.loc[datascanId.extension == "txt", 'name'].str.count("_log").sum()

        # count of scanID_meta.txt should be 1
        count_meta = datascanId.loc[datascanId.extension == "txt", 'name'].str.count("_meta").sum()

        # count of sampleID_fracID_raw_1.tif and sampleID_fracID_raw_1.zip should be 1 and/or 1
        count_raw_tif = datascanId.loc[datascanId.extension == "tif", 'name'].str.count("_raw").sum()
        raw_zip = datascanId.loc[datascanId.extension == "zip", ['name', 'inside_name']]
        count_raw_zip = raw_zip['name'].str.count("_raw").sum()

        # if a zip is present, check that the inside name is the same as the zip name
        if count_raw_zip == 1 and raw_zip['name'].values != raw_zip['inside_name'].values:
            result.loc[result["scan_id"] == id + "_1", 'raw_files'] += labels.errors["process.raw_files.rename_zip"]

        # if the count of files is as expected
        elif count_log == 1 and count_meta == 1 and (count_raw_tif == 1 or count_raw_zip == 1):
            result.loc[result["scan_id"] == id + "_1", 'raw_files'] = labels.sucess["process.raw_files.ok"]

        # if less than expected
        if count_log < 1 or count_meta < 1 or (count_raw_tif < 1 and count_raw_zip < 1):
            result.loc[result["scan_id"] == id + "_1", 'raw_files'] += labels.errors["process.raw_files.missing"]

        # if more than excepted
        if count_log > 1 or count_meta > 1 or count_raw_tif > 1 or count_raw_zip > 1:
            result.loc[result["scan_id"] == id + "_1", 'raw_files'] += labels.errors["process.raw_files.duplicate"]
    
    result.loc[result["raw_files"]=="", "raw_files"]=labels.errors["process.raw_files.inconsistent_scan_id"]
    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', 'raw_files': 'RAW files'}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_raw_files".format((time.time() - start_time), _id, _mode))
    return result

def check_scan_weight(_id, _mode, local_data):
    """All image files _raw_1.tif inside the _raw directory must be the same size.
    
    In the column "SCAN weight" of the report table:
        - "#BUG weight" : all these .tif have not the same weight
        - "Weight OK" : all these .tif have the same weight
    """
    start_time = time.time()

    # Get only usefull column size :  where path contains /_raw/ and extension == .tif
    fs = local_data.get("fs")
    dataToTest = fs.loc[([True if "/_raw/" in i else False for i in fs['path'].values]) & (fs['extension'].values == "tif"), "size"]
    result = local_data.get("dataframe")[['scan_id']].drop_duplicates()

    # check that all these tif have the same weight
    if len(dataToTest.unique()) == 1:
        result["scan_weight"] = labels.sucess["process.scan_weight.ok"]
    else:
        result["scan_weight"] = labels.errors["process.scan_weight.bug"]

    # Keep only one line by couples : id / scan weight
    result = result.drop_duplicates()

    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', 'scan_weight': 'SCAN weight'}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_scan_weight".format((time.time() - start_time), _id, _mode))
    return result

def check_process_post_scan(_id, _mode, local_data):
    """ Checks the file system structure post scan generated.

    In the _work directory of Zooscan_scan must be present :
        - N images named : scanID_X.jpg (images go from 1 to infinity)
        - 1 table ecotaxa_scanID.tsv
        - 1 file of the following types dat1.pid, msk1.gif, out1.gif
        - 1 zipped image vis1.zip

    In the column "POST SCAN" of the report table:
        - "#UNPROCESSED" : if less tsv than expected
        - "#DUPLICATE TSV" : if more tsv than excepted
        - "#MISSING ZIP" : if less zip than expected
        - "#DUPLICATE ZIP" : if more zip than excepted
        - "#bug RENAME ZIP FILE" : if a zip is present, but the inside name isn't the same as the zip name
        - "#BAD ZIP FILE" : if a zip is present, but this zip is corrupted and can't be read
        - "Process OK" : if the count of files is as expected
    """
    #JCE TODO "N images named" not tested
    start_time = time.time()

    # Get only usefull column size : where path contains /_raw/
    fs = local_data.get("fs")
    dataToTest = fs.loc[([True if "/_work/" in i else False for i in fs['path'].values]), ["path", "name", "extension", "inside_name"]]
    result = local_data.get("dataframe")[['scan_id']].drop_duplicates()
    result["process_post_scan"] = ""

    # Extract scan ids
    ids = result["scan_id"].values

    # foreatch scan id
    for id in ids:
        #get lines related to curent id
        datascanId = dataToTest.loc[([True if id in i else False for i in dataToTest['name'].values]), ["name", "extension", "inside_name"]]

        # count of scanID.tsv should be 1
        count_tsv = len(datascanId.loc[datascanId.extension == "tsv", 'name'])

        # count of scanID_vis1.zip should be 1
        work_zip = datascanId.loc[datascanId.extension== "zip", ['name', 'inside_name']]
        count_work_zip = len(work_zip['name'])

        # if a zip is present, check that this zip is not corrupted
        if count_work_zip == 1 and labels.errors["global.bad_zip_file"] == work_zip['inside_name'].values:
            result.loc[result["scan_id"] == id, 'process_post_scan'] += labels.errors["global.bad_zip_file"]

        # if a zip is present, check that the inside name is the same as the zip name
        elif count_work_zip == 1 and work_zip['name'].values+".tif" != work_zip['inside_name'].values:
            result.loc[result["scan_id"] == id, 'process_post_scan'] += labels.errors["process.post_scan.rename_zip"]

        # if the count of files is as expected
        elif count_work_zip == 1 and count_tsv == 1 :
            result.loc[result["scan_id"] == id, 'process_post_scan'] = labels.sucess["process.post_scan.ok"]

        else :
            # if less tsv than expected
            if count_tsv < 1:
                result.loc[result["scan_id"] == id, 'process_post_scan'] += labels.errors["process.post_scan.unprocessed"]
                
            # if more tsv than excepted
            if count_tsv > 1:
                result.loc[result["scan_id"] == id, 'process_post_scan'] += labels.errors["process.post_scan.duplicate.tsv"]

            # if less zip than expected
            if count_work_zip < 1:
                result.loc[result["scan_id"] == id, 'process_post_scan'] += labels.errors["process.post_scan.missing.zip"]

            # if more zip than excepted
            if count_work_zip > 1:
                result.loc[result["scan_id"] == id, 'process_post_scan'] += labels.errors["process.post_scan.duplicate.zip"]

    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', 'process_post_scan': 'POST SCAN'}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_process_post_scan".format((time.time() - start_time), _id, _mode))
    return result

def check_nb_lines_tsv(_id, _mode, local_data):
    """ Checks that the number of lines in the tsv file is consistent with the number of images in the related folder.

    In the column "Nb tsv lines" of the report table:
        - "#MISSING ecotaxa table" : if no ecotaxa_scanID.tsv table.
        - "#Images nb ≠ TSV lignes nb" : if the number of lines in a tsv file is différents than the number of images in the related folder
        - "Nb lines TSV OK" : if the count of lines and images are as expected
    """
    start_time = time.time()

    # Get only usefull data : where path contains /_raw/
    fs = local_data.get("fs")
    dataToTest = fs.loc[([True if ("/Zooscan_scan/_work/" in i and "multiples_to_separate" not in i)else False for i in fs['path'].values])
                        & (fs['extension'].values == "jpg"), ["name", "extension"]]

    result = local_data.get("dataframe")[['scan_id', 'object_id']].groupby("scan_id").size().reset_index(name='nb_objects')
    result["nb_lines_tsv"] = ""
    
    temp =  local_data.get("dataframe")[['scan_id', 'object_id']].groupby('scan_id').first().reset_index()

    # Extract scan ids
    ids = result["scan_id"].values

    # foreatch scan id
    for id in ids:
        #get lines related to curent id
        data_img_list = dataToTest.loc[([True if id in i else False for i in dataToTest['name'].values]), ["name", "extension"]]

        # count number of img in work
        count_img = len(data_img_list)

        # get number of object for id
        count_tsv_lines = result[result["scan_id"]==id]["nb_objects"].values[0]
        # if missing TSV
        if temp[temp["scan_id"] == id]["object_id"].values[0]==labels.errors["global.missing_ecotaxa_table"] : 
            result.loc[result["scan_id"] == id, 'nb_lines_tsv'] += labels.errors["global.missing_ecotaxa_table"]
        # if nb of img == nb of object in tsv
        elif count_img == count_tsv_lines :
            result.loc[result["scan_id"] == id, 'nb_lines_tsv'] += labels.sucess["process.nb_lines_tsv.ok"]
        # if nb of img != nb of object in tsv
        else :
            result.loc[result["scan_id"] == id, 'nb_lines_tsv'] += labels.errors["process.nb_lines_tsv.diff"]
            
                
    #drop usless collumns
    result.drop(columns=["nb_objects"], inplace=True)

    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', 'nb_lines_tsv': 'Nb tsv lines'}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_nb_lines_tsv".format((time.time() - start_time), _id, _mode))
    return result

def check_bw_ratio(_id, _mode, local_data):
    """In order to ensure the quality of the process, the value of the B/W ratio must be strictly less than 0.25.

        In the column "B/W ratio" of the report table:
            - "#MISSING ecotaxa table" : if no ecotaxa_scanID.tsv table.
            - "#MISSING column" : if process_particle_bw_ratio column is missing from the ecotaxa.tsv table.
            - "#Ratio NOK" :  if the value of the process_particle_bw_ratio in the ecotaxatable.tsv is out of range or not even a number.
            - "Ratio OK" : if the value of the process_particle_bw_ratio in the ecotaxatable.tsv is between 0 and 0,25.
    """
    start_time = time.time()

    # Get only usefull columns
    result = local_data.get("dataframe")[['scan_id', 'process_particle_bw_ratio']].drop_duplicates()

    # Replace by ratio OK or associated error code
    result.process_particle_bw_ratio = result.process_particle_bw_ratio.map(lambda x: x if labels.errors["global.missing_ecotaxa_table"] == x
                                                                            else x if x==labels.errors["global.missing_column"]
                                                                            else labels.sucess["process.bw_ratio.ok"] if is_float(x) and float(x) < 0.25 and float(x) > 0
                                                                            else labels.errors["process.bw_ratio.not_ok"])

    # Keep only one line by couples : id / ratio
    result = result.drop_duplicates()

    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', 'process_particle_bw_ratio': 'B/W ratio'}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_bw_ratio".format((time.time() - start_time), _id, _mode))
    return result

def check_pixel_size(_id, _mode, local_data):
    """The idea here is to reveal an old zooprocess bug that was mistaken about the pixel size to apply for morphometric calculations. The purpose is to check that the pixel_size is consistent with the process_img_resolution. 

        It should match one of the following proposals :
            - size :"0.0847" and resolution : "300"
            - size :"0.0408" and resolution : "600" 
            - size :"0.0204" and resolution : "1200"
            - size :"0.0106" and resolution : "2400"
            - size :"0.0053" and resolution : "4800"
            
        In the column "PIXEL size" of the report table:
            - "#MISSING ecotaxa table" : if no ecotaxa_scanID.tsv table.
            - "#MISSING column" : if pixel_size column  or process_img_resolution column is missing from the ecotaxa.tsv table.
            - "#Size NOK" : if the pixel size is not consistent with the resolution.
            - pixel size value : if the pixel size is consistent with the resolution.
    """
    start_time = time.time()

    # Get only usefull columns
    dataToTest = local_data.get("dataframe")[['scan_id', 'process_particle_pixel_size_mm', 'process_img_resolution']].groupby('scan_id').first().reset_index()
    result = local_data.get("dataframe")[['scan_id']].drop_duplicates()
    result["pixel_size"] = ""

    for i in range(0, len(dataToTest.scan_id)):
        id = dataToTest.scan_id.values[i]
        size = dataToTest.process_particle_pixel_size_mm.values[i]
        resolution = dataToTest.process_img_resolution.values[i]
        
        if((size == "0.0847" and resolution == "300") 
        or (size == "0.0408" and resolution == "600") 
        or (size == "0.0204" and resolution == "1200")
        or (size == "0.0106" and resolution == "2400")
        or (size == "0.0053" and resolution == "4800")) : 
            result.loc[result["scan_id"] == id, 'pixel_size'] = size
        else :
            result.loc[result["scan_id"] == id, 'pixel_size'] = labels.errors["global.missing_ecotaxa_table"] if size == labels.errors["global.missing_ecotaxa_table"] \
                                                                else labels.errors["global.missing_column"] if size==labels.errors["global.missing_column"] \
                                                                else labels.errors["process.pixel_size.not_ok"]

    # Keep only one line by couples : id / is resolution coerent
    result = result.drop_duplicates()

    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', 'pixel_size': 'PIXEL size'}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_pixel_size".format((time.time() - start_time), _id, _mode))
    return result

def check_sep_mask(_id, _mode, local_data):
    """Verification of the presence of a sep.gif mask in the subdirectory of the _work. 

        In the column "SEP MASK" of the report table:
            - "#MISSING SEP MSK = (F)" : If the sep.gif mask is not present, indicate also the motoda fraction associated with the scan to eliminate the situation where there was no multiple to separate because the sample was very poor and therefore motoda = 1
            - "Sep mask OK" : If a sep.gif mask is present
    """
    start_time = time.time()

    # Get only usefull column :  where path contains /_work/ and extension == .gif
    fs = local_data.get("fs")
    dataToTest = fs.loc[([True if "/_work/" in i and "_sep" in i else False for i in fs['path'].values])
                        & (fs['extension'].values == "gif"), ["name", "extension"]].name.values

    result = local_data.get("dataframe")[['scan_id', 'acq_sub_part']].groupby('scan_id').first().reset_index()
    result["sep_mask"] = ""
    for id in result.scan_id:
        if id + "_sep" in dataToTest:
            result.loc[result["scan_id"] == id, 'sep_mask'] += labels.sucess["process.sep_mask.ok"]
        else:
            # get motoda frac from data
            motoda_frac = result.loc[result["scan_id"] == id, 'acq_sub_part']
            result.loc[result["scan_id"] == id, 'sep_mask'] = labels.errors["process.sep_mask.missing"] + " = " + motoda_frac

    result.drop(columns=["acq_sub_part"], inplace=True)

    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', 'sep_mask': 'SEP MASK'}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_sep_mask".format((time.time() - start_time), _id, _mode))
    return result

def check_process_post_sep(_id, _mode, local_data):
    """ This second process must include the separation mask (if any) created in the previous step.

        In the column "POST SEP" of the report table:
            - "#UNPROCESSED" : if missing ecotaxa_scanID.tsv table.
            - "#MISSING column" : if process_particle_sep_mask column is missing from the ecotaxa.tsv table.
            - "#SEP MSK NOT INCLUDED" : if process_particle_sep_mask from ecotaxa.tsv table does not contains "include"
            - "process OK" : if process_particle_sep_mask from ecotaxa.tsv table contains "include"
    """
    start_time = time.time()

    # Get only usefull column : scan_id and process_particle_sep_mask
    result = local_data.get("dataframe")[['scan_id', 'process_particle_sep_mask']].drop_duplicates()

    # Replace by large or narrow or associated error code
    result.process_particle_sep_mask = result.process_particle_sep_mask.map(lambda x: labels.sucess["process.post_sep.ok"] if "include" in x
                                                                            else labels.errors["process.post_sep.unprocessed"] if labels.errors["global.missing_ecotaxa_table"] == x
                                                                            else x if x==labels.errors["global.missing_column"]
                                                                            else labels.errors["process.post_sep.not_included"])

    # Keep only one line by couples : id / frame type
    result = result.drop_duplicates()

    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', 'process_particle_sep_mask': 'POST SEP'}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_process_post_sep".format((time.time() - start_time), _id, _mode))
    return result

### ACQUISITION

def check_sieve_bug(_id, _mode, local_data):
    """This check verifies the sieve values used to prepare the scan. They must be constant in the same project and therefore NEVER be different.
    
    In the columns 'sieve MIN size' and 'sieve MAX size' of the report table, are reported respectively :
        - the minimum sieve size, column CS acq_min_mesh of the ecotaxa_scanID.tsv tables of the subdirectories of the _work directories
        - the maximum size, column CT acq_max_mesh of the tables ecotaxa_scanID.tsv of the sub-directories of the directories _work

    In the 'sieve BUG' column :
        - "#MISSING ecotaxa table" : If no ecotaxa_scanID.tsv table
        - "#NOT NUMERIC" : If the acq_min and acq_max values are not numerical
        - "#SIEVE different from others" : If the acq_min of 1 or more scans differs from the other scans with the same frac ID or the acq_max of 1 or more scans diverges from the other scans with the same frac ID
        - "#ACQ MIN > ACQ MAX" : If the acq_min is greater than the acq_max for the same FracID (within the same scanID)
        - "#ACQ MIN = ACQ MAX" : If the acq_min is equal to the acq_max for the same FracID (within the same scanID)
        - "#ACQ MIN (dN) ≠ ACQ MAX (dN+1)" : if for the same sampleID whose FracID = d1 or d2 or d3 (comparison between several scanIDs of the same sampleID) the acq_min (dN) ≠ acq_max (d+1)
        - "sieve OK" : If everything is OK
    """
    start_time = time.time()

    # Get only usefull columns
    result = local_data.get("dataframe")[['scan_id', 'acq_min_mesh', 'sample_id', 'acq_max_mesh']].drop_duplicates()
    result["fracID"] = [get_frac_id(e) for e in result["scan_id"]]
    result['sieve_bug']=""

    # Replace by sieve OK or associated error code
    result.sieve_bug = result.acq_min_mesh.map(lambda x: x if labels.errors["global.missing_ecotaxa_table"] == x
                                                           else labels.errors["global.not_numeric"] if not is_int(x)
                                                           else labels.sucess["acquisition.sieve.bug.ok"]) 
    result.loc[result["acq_max_mesh"].apply(is_not_int) & result["sieve_bug"]==labels.sucess["acquisition.sieve.bug.ok"], 'sieve_bug']=labels.errors["global.not_numeric"]

    # cast the type
    result=result.astype({"acq_min_mesh" : "int", "acq_max_mesh" : "int"}, errors='ignore')

    # The acq_min is superior or equal to the acq_max **for the same FracID (within the same scanID)**, 
    # put a warning "ACQ MIN > ACQ MAX" or "ACQ MIN = ACQ MAX" according to the situation:
    for id in result.scan_id : 
        sieve_bug_label=result.loc[result["scan_id"] == id, 'sieve_bug'].values[0]
        if sieve_bug_label == labels.sucess["acquisition.sieve.bug.ok"] :
            acq_min_mesh = int(result.loc[result["scan_id"] == id, 'acq_min_mesh'].values[0]) if is_int(result.loc[result["scan_id"] == id, 'acq_min_mesh'].values[0]) else result.loc[result["scan_id"] == id, 'acq_min_mesh'].values[0]
            acq_max_mesh = int(result.loc[result["scan_id"] == id, 'acq_max_mesh'].values[0]) if is_int(result.loc[result["scan_id"] == id, 'acq_max_mesh'].values[0]) else result.loc[result["scan_id"] == id, 'acq_max_mesh'].values[0]
            
            # If acq_min_mesh == acq_max_mesh 
            if acq_min_mesh == acq_max_mesh :
                result.loc[result["scan_id"] == id, 'sieve_bug'] = labels.errors["acquisition.sieve.bug.min_equ_max"]
            # If acq_min_mesh > acq_max_mesh 
            elif acq_min_mesh > acq_max_mesh :
                result.loc[result["scan_id"] == id, 'sieve_bug'] = labels.errors["acquisition.sieve.bug.min_sup_max"]

    ## TODO JCE  
    #For the same sampleID whose FracID = d1 or d2 ... dN (comparison between several scanIDs of the same sampleID), check the following conditions:
    # - the acq_min (dN) ≠ acq_max (dN+1), put a warning "ACQ MIN (dN) ≠ ACQ MAX (dN+1)"
    data_by_sample_id = result.groupby("sample_id")
    for key, item in data_by_sample_id:
        group=data_by_sample_id.get_group(key)
        if len(group)>1 :
            for i in range(1,len(group)) :
                #Get d_i and d_i+1
                d_i=group[group["fracID"]=="_d"+str(i)+"_"]
                d_i_plus_1=group[group["fracID"]=="_d"+str(i+1)+"_"]

                #if not d_i and d_i+1 skip test
                if d_i.empty or d_i_plus_1.empty : 
                    break

                #Compare d_i and d_i+1
                #Should respect "acq_min_mesh (N) == acq_max_mesh (N+1)"
                di_acq_min_mesh = int(d_i.acq_min_mesh.values[0]) if is_int(d_i.acq_min_mesh.values[0]) else d_i.acq_min_mesh.values[0]
                di_plus_un_acq_max_mesh = int(d_i_plus_1.acq_max_mesh.values[0]) if is_int(d_i_plus_1.acq_max_mesh.values[0]) else d_i_plus_1.acq_max_mesh.values[0]
                if di_acq_min_mesh != di_plus_un_acq_max_mesh :
                    result.loc[result["scan_id"] == d_i.scan_id.values[0], 'sieve_bug'] = labels.errors["acquisition.sieve.bug.min_dn_dif_max_dn+1_1"]+str(i)+labels.errors["acquisition.sieve.bug.min_dn_dif_max_dn+1_2"]+str(i+1)+labels.errors["acquisition.sieve.bug.min_dn_dif_max_dn+1_3"]
                    result.loc[result["scan_id"] == d_i_plus_1.scan_id.values[0], 'sieve_bug'] = labels.errors["acquisition.sieve.bug.min_dn_dif_max_dn+1_1"]+str(i)+labels.errors["acquisition.sieve.bug.min_dn_dif_max_dn+1_2"]+str(i+1)+labels.errors["acquisition.sieve.bug.min_dn_dif_max_dn+1_3"]
    
    #For the same handled Frac ID 
    data_by_frac_id = result.groupby("fracID")
    for key, item in data_by_frac_id:
        if key!= "frac_type_not_handled" :
            group=data_by_frac_id.get_group(key)
            # If the acq of one or more scans differs from the other scans
            if len(group['acq_min_mesh'].unique())>1 or len(group['acq_max_mesh'].unique())>1 :
                result.loc[result['fracID'] == key,  'sieve_bug'] = labels.errors["acquisition.sieve.bug.different"]+" ("+key.replace('_', '')+")"
    
    # Keep only one usfull lines
    result = result.drop_duplicates()
    result.drop(columns=["sample_id", "fracID"], inplace=True)

    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', 'acq_min_mesh': 'acq min mesh', 'acq_max_mesh': 'acq max mesh', 'sieve_bug' : 'Sieve Bug'}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_sieve_bug".format((time.time() - start_time), _id, _mode))
    return result

def check_motoda_check(_id, _mode, local_data):
    """This control performs a numerical check on the motoda fraction used. 
    This quality test identifies data generated with an earlier version of Zooprocess that did not take motoda into account. The ecotaxa.tsv tables all had the same motoda fraction number.

        In the column 'MOTODA Fraction' of the report table, is reported :
            - the fraction acq_sub_part of the tables ecotaxa_scanID.tsv of the subdirectories of the _work directories.

        In the column 'MOTODA check' of the report table :
            - "#NOT NUMERIC": if the acq_sub_part value is not numeric
            - "#MISSING ecotaxa table" : if no ecotaxa_scanID.tsv table
            - "#Motoda Fraction ≠ 1 or ≠ ^2": if when FracID = d1 regardless of sample_net_type OR when FracID = tot and sample_net_type = rg, 
              does not respect → acq_sub_part = 1 or a power of 2
            - "#Motoda Fraction ≠ ^2": if when FracID = dN+1 OR =tot OR = plankton (all net types except rg), 
              does not respect → acq_sub_part = a power of 2 except 1
            - "#Motoda identical": if acq_sub_part is identical throughout the project
            - "Motoda OK" : if everything is OK
    """
    start_time = time.time()
    # Get only usefull columns
    dataToTest = local_data.get("dataframe")[['scan_id', 'acq_sub_part', 'sample_net_type']].groupby('scan_id').first().reset_index()
    dataToTest["fracID"] = [get_frac_id(e) for e in dataToTest["scan_id"]]
    result = local_data.get("dataframe")[['scan_id','acq_sub_part']].drop_duplicates()
    result["motoda_check"] = ""

    # fill with motoda OK or associated generic error code
    result.motoda_check = result.acq_sub_part.map(lambda x: x if labels.errors["global.missing_ecotaxa_table"] == x
                                                           else labels.errors["global.not_numeric"] if not is_int(x)
                                                           else labels.sucess["acquisition.motoda.check.ok"]) 
    # If Motoda identique
    if len(dataToTest.acq_sub_part.unique()) == 1 :
        result['motoda_check'] =  np.where(result['motoda_check'] == labels.sucess["acquisition.motoda.check.ok"],labels.errors["acquisition.motoda.check.identique"], result['motoda_check'])
    nb_scans = len(dataToTest.scan_id)
    if (nb_scans>2) :
        for i in range(0, nb_scans):
            id = dataToTest.scan_id.values[i]
            motoda_check = result.loc[result["scan_id"] == id, 'motoda_check'].values[0]
            sample_net_type = dataToTest.sample_net_type.values[i]
            fracID = dataToTest.fracID.values[i]
            acq_sub_part = dataToTest.acq_sub_part.values[i]

            if motoda_check == labels.sucess["acquisition.motoda.check.ok"]:
                result.loc[result["scan_id"] == id, 'acq_sub_part'] = int(acq_sub_part) if is_int(acq_sub_part) else acq_sub_part
                if(fracID=="_d1_" or ( fracID=="_tot_" and sample_net_type=="rg")) :
                    #should be (1 or )puissance de 2
                    if not is_power_of_two(int(acq_sub_part)) :
                        result.loc[result["scan_id"] == id, 'motoda_check'] = labels.errors["acquisition.motoda.check.cas1"]
                elif (fracID.startswith("_d") or fracID=="_tot_" or fracID=="_plankton_") and sample_net_type != "rg"  :
                    if int(acq_sub_part)==1 or not is_power_of_two(int(acq_sub_part)) :
                        #should be ^2 but not 1
                        result.loc[result["scan_id"] == id, 'motoda_check'] = labels.errors["acquisition.motoda.check.cas2"]

    # Keep only one line by couples : id / motoda fraction
    result = result.drop_duplicates()

    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', 'acq_sub_part' : 'MOTODA Fraction','motoda_check': 'MOTODA check'}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_motoda_check".format((time.time() - start_time), _id, _mode))
    return result

def check_motoda_comparaison(_id, _mode, local_data):
    """ Comparison of the motoda fraction between scanIDs of the same sampleID, this control is only done for scans where the FracID = d1 or d2 or dn... , so having the same sampleID.

        In the column 'MOTODA comparison' of the report table :
            - "#NOT NUMERIC": if the acq_sub_part value is not numeric
            - "#MISSING ecotaxa table": if no ecotaxa_scanID.tsv table
            - "#Motoda frac (dN-1) > Motoda frac (dN)": if does not respect acq_sub_part (N) <= acq_sub_part (N+1). 
              Example : acq_sub_part (d1) > acq_sub_part (d2)
            - "#Motoda comparison OK" : if everything is OK
            
        In the columns 'Sample Comment' and 'Observation' of the report table, are reported respectively :
            - the sample_comment, column EK of the ecotaxa_scanID.tsv tables of the subdirectories of the _work directories. 
              If the ecotaxa.tsv table is missing it is retrieved from the meta.txt.
            - the Observation, from the meta.txt file of the _work directories.
    """
    start_time = time.time()
    # Get only usefull columns
    dataToTest = local_data.get("dataframe")[['scan_id', 'acq_sub_part', 'sample_id']].groupby('scan_id').first().reset_index()
    dataToTest["fracID"] = [get_frac_id(e) for e in dataToTest["scan_id"]]
    result = local_data.get("dataframe")[['scan_id','acq_sub_part', 'sample_comment', 'sample_id']].drop_duplicates()
    result.insert(loc=2, column='motoda_comp', value="")
    result['Observation']=""
    
    #get meta.txt related information
    meta = local_data.get("meta")

    # fill with motoda OK or associated generic error code
    result.motoda_comp = result.acq_sub_part.map(lambda x: x if labels.errors["global.missing_ecotaxa_table"] == x
                                                           else labels.errors["global.not_numeric"] if not is_int(x)
                                                           else labels.sucess["acquisition.motoda.comparaison.ok"]) 

    data_by_sample_id = dataToTest.groupby("sample_id")
    
    for key, item in data_by_sample_id:
        group=data_by_sample_id.get_group(key)

        if len(group)>1 :
            for i in range(1,len(group)) :
                #Get d_i and d_i+1
                d_i=group[group["fracID"]=="_d"+str(i)+"_"]
                d_i_plus_1=group[group["fracID"]=="_d"+str(i+1)+"_"]

                #if not d_i and d_i+1 skip test
                if d_i.empty or d_i_plus_1.empty : 
                    break

                #Compare d_i and d_i+1
                #Should respect "acq_sub_part (N) <= acq_sub_part (N+1)"
                di_acq_sub_part = int(d_i.acq_sub_part.values[0]) if is_int(d_i.acq_sub_part.values[0]) else d_i.acq_sub_part.values[0]
                di_plus_un_acq_sub_part = int(d_i_plus_1.acq_sub_part.values[0]) if is_int(d_i_plus_1.acq_sub_part.values[0]) else d_i_plus_1.acq_sub_part.values[0]
                if di_acq_sub_part > di_plus_un_acq_sub_part :
                    result.loc[result["scan_id"] == d_i.scan_id.values[0], 'motoda_comp'] = labels.errors["acquisition.motoda.comparaison.ko"]+" (d"+str(i)+") > Motoda frac (d"+str(i+1)+")"
                    result.loc[result["scan_id"] == d_i_plus_1.scan_id.values[0], 'motoda_comp'] = labels.errors["acquisition.motoda.comparaison.ko"]+" (d"+str(i)+") > Motoda frac (d"+str(i+1)+")"
    
    # Extract scan ids
    ids = result["scan_id"].values
    if len(meta) == 0 :
        result['sample_comment'] = labels.errors["global.missing_meta_txt_file"]
        result['Observation'] = labels.errors["global.missing_meta_txt_file"]
    else :
        # foreatch scan id
        for id in ids:
            #get meta related to curent scan_id (opti)
            for k,v in meta.items() :
                if id in k :
                    meta_for_scan_id = v
                    break
                
            if result.loc[result["scan_id"] == id ].motoda_comp.values[0] == labels.errors["global.missing_ecotaxa_table"] :
                #get sample_comment
                meta_sample_comment =  meta_for_scan_id if meta_for_scan_id == labels.errors["global.bad_meta_txt_file"] else [a.replace("Sample_comment= ", "") for a in meta_for_scan_id if a.startswith("Sample_comment= ")]
                #set sample_comment
                result.loc[result["scan_id"] == id, "sample_comment"] = meta_sample_comment[0] if len(meta_sample_comment)>0 else labels.errors["global.missing_column"]+" in meta.txt"
            
            #get and set Observation
            meta_observation =  meta_for_scan_id if meta_for_scan_id == labels.errors["global.bad_meta_txt_file"] else [a.replace("Observation= ", "") for a in meta_for_scan_id if a.startswith("Observation= ")]
            result.loc[result["scan_id"] == id, "Observation"] = meta_observation[0] if len(meta_observation)>0 else labels.errors["global.missing_column"]+"  in meta.txt"

    # Keep only one line by couples : id / motoda fraction
    result = result.drop_duplicates()
    result.drop(columns=["acq_sub_part", "sample_id"], inplace=True)

    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', 'motoda_comp': 'MOTODA comparison', 'sample_comment':'Sample comment'}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_motoda_comparaison".format((time.time() - start_time), _id, _mode))
    return result

def check_motoda_quality(_id, _mode, local_data):
    """ .jpg images, commonly called vignettes, are created in the sub-directories of the _work directory following the initial process step. The number of vignettes created tells us about the quality of the fraction chosen with the motoda to make the scan : sample with insufficient or too many splits.
        
        In the column 'Motoda quality' of the report table:
            - "#NOT NUMERIC": if the acq_sub_part value is not numeric
            - "#MISSING ecotaxa table": if no ecotaxa_scanID.tsv table
            - "#MISSING images" : if No .jpg images in the sub-directories of the _work directory
            - "#Images nb LOW : N" or "#Images nb HIGH : N" : if the following conditions are not met :
                    When Nettype is rg and motoda_frac strictly equal to 1
                        → the number of .jpg images in the _work subdirectory must not be > 1500
                    When Nettype is rg and motoda_frac strictly > 1
                        → the number of .jpg images in the _work subdirectory must be between 500 and 1500
                    When Nettype ≠ rg and FracID = d1 and motoda_frac strictly equal to 1
                        → the number of .jpg images in the _work subdirectory must not be > 1500
                    When Nettype ≠ rg and FracID = d1 and the motoda_frac strictly > 1 
                        → the number of .jpg images in the _work subdirectory must be between 500 and 1500
                    When Nettype ≠ rg and FracID = d1+N or = tot or =plankton and motoda_frac strictly equal to 1
                        → the number of .jpg images in the _work subdirectory must not be > 2500
                    When Nettype ≠ rg and FracID = d1+N or = tot or =plankton and motoda_frac strictly >1
                        → the number of .jpg images in the _work subdirectory must be between 1000 and 2500
            - "Motoda OK" : if everything is OK
    """
    start_time = time.time()
    # Get only usefull columns
    result = local_data.get("dataframe")[['scan_id', 'sample_net_type', 'acq_sub_part']].drop_duplicates()
    result["fracID"] = [get_frac_id(e) for e in result["scan_id"]]
    result['motoda_quality']=""
    # Get only usefull file name : .jpg in /Zooscan_scan/_work/ folder
    fs = local_data.get("fs")
    dataToTest = fs.loc[([True if ("/Zooscan_scan/_work/" in i and "multiples_to_separate" not in i)else False for i in fs['path'].values])
                        & (fs['extension'].values == "jpg"), ["name", "extension"]]

    # fill with motoda OK or associated generic error code
    result.motoda_quality = result.acq_sub_part.map(lambda x: x if labels.errors["global.missing_ecotaxa_table"] == x
                                                           else labels.errors["global.not_numeric"] if not is_int(x)
                                                           else "") 
    # Extract scan ids
    ids = result["scan_id"].values

    # foreatch scan id
    for id in ids:
        #get lines related to curent id
        data_img_list = dataToTest.loc[([True if id in i else False for i in dataToTest['name'].values]), ["name", "extension"]]

        # count of scanID.tsv should be 1
        count_img = len(data_img_list)
        #get needed infos for that scan id
        scan_id_data = result.loc[result["scan_id"]==id, ["sample_net_type", "fracID", "acq_sub_part", "motoda_quality"]]
        # if no img in work, fill result with the associated error msg
        if count_img==0 :
            result.loc[result["scan_id"] == id, 'motoda_quality'] = labels.errors["acquisition.motoda.quality.missing"]
        elif scan_id_data["motoda_quality"].values[0]=="" :
            #get needed infos for that scan id
            net_type = scan_id_data["sample_net_type"].values[0]
            frac_id = scan_id_data["fracID"].values[0]
            motoda_frac = int(scan_id_data["acq_sub_part"].values[0]) if is_int(scan_id_data["acq_sub_part"].values[0]) else scan_id_data["acq_sub_part"].values[0]

            #start testing
            if net_type=="rg" :
                # When Nettype = rg and motoda_frac strictly =1
                if motoda_frac==1 :
                    # the number of .jpg images in the _work subdirectory must not be > 1500
                    result.loc[result["scan_id"] == id, 'motoda_quality'] = labels.sucess["acquisition.motoda.quality.ok"] if count_img <= 1500 else labels.errors["acquisition.motoda.quality.high"]+str(count_img)
                # When Nettype = rg and motoda_frac strictly >1
                elif motoda_frac>1 :
                    # the number of .jpg images in the _work subdirectory must be between 500 and 1500
                    result.loc[result["scan_id"] == id, 'motoda_quality'] = labels.errors["acquisition.motoda.quality.low"]+str(count_img) if count_img < 500 else labels.errors["acquisition.motoda.quality.high"]+str(count_img) if count_img > 1500 else labels.sucess["acquisition.motoda.quality.ok"] 
                else : 
                    result.loc[result["scan_id"] == id, 'motoda_quality'] = labels.sucess["acquisition.motoda.quality.ok"]
            #For all other Nettype:
            else :
                if frac_id=="_d1_" : 
                    # When Nettype ≠ rg and FracID = d1 and motoda_frac strictly =1
                    if motoda_frac == 1 :
                        # the number of .jpg images in the _work subdirectory must not be > 1500
                        result.loc[result["scan_id"] == id, 'motoda_quality'] = labels.sucess["acquisition.motoda.quality.ok"] if count_img <= 1500 else labels.errors["acquisition.motoda.quality.high"]+str(count_img)
                    # When Nettype ≠ rg and FracID = d1 and the motoda_frac strictly >1 
                    elif motoda_frac > 1 :
                        # the number of .jpg images in the _work subdirectory must be between 500 and 1500
                        result.loc[result["scan_id"] == id, 'motoda_quality'] = labels.errors["acquisition.motoda.quality.low"]+str(count_img) if count_img < 500 else labels.errors["acquisition.motoda.quality.high"]+str(count_img) if count_img > 1500 else labels.sucess["acquisition.motoda.quality.ok"] 
                
                elif (frac_id.startswith("_d") or frac_id=="_tot_" or frac_id=="_plankton_") : 
                    # When Nettype ≠ rg and FracID = d1+N or = tot or =plankton and motoda_frac strictly =1
                    if motoda_frac == 1 :
                        # the number of .jpg images in the _work subdirectory must not be > 2500
                        result.loc[result["scan_id"] == id, 'motoda_quality'] = labels.sucess["acquisition.motoda.quality.ok"] if count_img <= 2500 else labels.errors["acquisition.motoda.quality.high"]+str(count_img)
                    # When Nettype ≠ rg and FracID = d1+N or = tot or =plankton and motoda_frac strictly >1
                    elif motoda_frac > 1 :
                        # the number of .jpg images in the _work subdirectory must be between 1000 and 2500
                        result.loc[result["scan_id"] == id, 'motoda_quality'] = labels.errors["acquisition.motoda.quality.low"]+str(count_img) if count_img < 1000 else labels.errors["acquisition.motoda.quality.high"]+str(count_img) if count_img > 2500 else labels.sucess["acquisition.motoda.quality.ok"] 
                else : 
                    result.loc[result["scan_id"] == id, 'motoda_quality'] = labels.sucess["acquisition.motoda.quality.ok"]

    #Remove result useless columns
    result.drop(columns=["fracID", "sample_net_type", "acq_sub_part"], inplace=True)

    # Rename collums to match the desiered output
    result.rename(columns={'scan_id': 'List scan ID', "motoda_quality" : "Motoda quality"}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_motoda_quality".format((time.time() - start_time), _id, _mode))
    return result

def check_spelling(_id, _mode, local_data):
    """Highlighting in a second table the spelling errors on fields that should be constant in the project:
        - analysis operator in "Scan op." column
        - splitting method in "Submethod" column
    """
    start_time = time.time()
    # Get only usefull columns and drp duplicate to keep uniques values
    result_sample_scan_operator = local_data.get("dataframe")[['sample_scan_operator']].drop_duplicates()
    result_acq_sub_method = local_data.get("dataframe")[['acq_sub_method']].drop_duplicates()

    #set same len to both col
    result = independent_concat(result_sample_scan_operator, "sample_scan_operator",result_acq_sub_method, "acq_sub_method") if len(result_sample_scan_operator["sample_scan_operator"])>len(result_acq_sub_method["acq_sub_method"]) else independent_concat(result_sample_scan_operator, "sample_scan_operator", result_acq_sub_method, "acq_sub_method") 

    # Rename collums to match the desiered output
    result.rename(columns={'sample_scan_operator': 'Scan op.', "acq_sub_method" : "Submethod"}, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_spelling".format((time.time() - start_time), _id, _mode))
    return result

def checks_multiples(_id, _mode, local_data) : 
    """ Checks that: all multiples < 20% of living, other multiples < 15% of (living - Copepoda), and Copepoda multiples < 15% of total Copepoda, all in both number (n) and biovolume (vol);
        NB: morphological sub-categories such as "head", "part", "dead", "Insecta", "wing" and "seaweed" are never counted.
        This QC is based on the newest ecotaxa default export zip file located in the "ecotaxa/" folder of your project.

        In followings coluns of the report table, are respectively reported :
            - List scan ID : the scan id.
            - "% mult" : %Ab multiples (other+cop) / living, should be < 20% of (total living) abundance
            - "%vol mult" : %Bv multiples (other+cop) / living, should be < 20% of (total living) biovolume
            - "% mult (non cop)" : %Ab multiples other / (living - cop), should be < 15% of (living - Copepoda et enfants) abundance
            - "%vol mult (non cop)" : %Bv multiples other / (living - cop), should be < 15% of (living - Copepoda et enfants) biovolume
            - "% mult (cop)" : %Ab multiples cop / (living - other), should be < 15% of total abundance
            - "%vol mult (cop)" : %Bv multiples cop / (living - other), should be < 15% of total biovolume

        In the above listed columns of the report table can appear an error code:
            - "#HIGH multiples level" : it means that the % of multiples is to high for the related sample.

    """
    start_time = time.time()
    # Get only usefull columns
    dataToTest = local_data.get("dataframe")[["object_id",'object_annotation_hierarchy', 'object_annotation_category', 'object_area']]

    # Remove not relevant categories
    dataToTest = dataToTest[dataToTest['object_annotation_hierarchy'].str.startswith("not-living")== False]
    # morpho stuff
    dataToTest = dataToTest[dataToTest['object_annotation_category'].str.contains("head")== False]
    dataToTest = dataToTest[dataToTest['object_annotation_category'].str.contains("part")== False]
    dataToTest = dataToTest[dataToTest['object_annotation_category'].str.contains("trunk")== False]
    dataToTest = dataToTest[dataToTest['object_annotation_category'].str.contains("dead")== False]
    # not plankton
    dataToTest = dataToTest[dataToTest['object_annotation_category'].str.contains("Insecta")== False]
    dataToTest = dataToTest[dataToTest['object_annotation_category'].str.contains("wing")== False]
    dataToTest = dataToTest[dataToTest['object_annotation_category'].str.contains("seaweed")== False]

    # Compute scan id and biovolume
    # The math.pi constant returns the value of PI: 3.141592653589793. The python pi const returns the value of PI : 3.141593 
    dataToTest["vol"]= [4/3 * 3.141593 * math.sqrt(float(area) / 3.141593) for area in dataToTest.object_area]#maybe here
    dataToTest["scan_id"]=dataToTest.object_id.str.replace("_1_[0-9]+$", "", regex=True)

    # Compute statistics by scan id  # missing means not counted = 0
    result = dataToTest.drop_duplicates().groupby("scan_id").agg( vol_tot=('vol', 'sum'), n_tot=('object_id', np.size)
        ).join(dataToTest[dataToTest["object_annotation_category"].str.contains("multiple")].groupby("scan_id").agg( vol_mult=('vol', 'sum'), n_mult=('object_id', np.size))
        ).join(dataToTest[dataToTest["object_annotation_hierarchy"].str.contains("Copepoda")].groupby("scan_id").agg( vol_cop=('vol', 'sum'), n_cop=('object_id', np.size))
        ).join(dataToTest[dataToTest["object_annotation_hierarchy"].str.contains("Copepoda>multiple")].groupby("scan_id").agg( vol_mult_cop=('vol', 'sum'), n_mult_cop=('object_id', np.size))
        ).join( dataToTest[dataToTest["object_annotation_hierarchy"].str.contains("other>multiple")].groupby("scan_id").agg( vol_mult_other=('vol', 'sum'), n_mult_other=('object_id', np.size))
        ).fillna(0)


    # Compute percent multiples
    result['p_mult'] = result.apply(lambda row: 
                                        labels.errors["multiples.level.high"]+str(round(row.n_mult / row.n_tot * 100, 1)) 
                                        if (row.n_mult / row.n_tot) > 0.2 
                                        else round(row.n_mult / row.n_tot * 100, 1), axis=1)
    result['p_vol_mult'] = result.apply(lambda row: 
                                        labels.errors["multiples.level.high"]+str( round(row.vol_mult / row.vol_tot * 100, 1)) 
                                        if (row.vol_mult / row.vol_tot) > 0.2 
                                        else round(row.vol_mult / row.vol_tot * 100, 1), axis=1)
    result['p_mult_non_cop'] = result.apply(lambda row: 
                                        labels.errors["multiples.level.high"]+str(round(row.n_mult_other /(row.n_tot - row.n_cop) * 100, 1)) 
                                        if (row.n_mult_other /(row.n_tot - row.n_cop)) > 0.15 
                                        else round(row.n_mult_other /(row.n_tot - row.n_cop) * 100, 1), axis=1)
    result['p_vol_mult_non_cop'] = result.apply(lambda row: 
                                        labels.errors["multiples.level.high"]+str(round(row.vol_mult_other / (row.vol_tot - row.vol_cop) * 100, 1)) 
                                        if (row.vol_mult_other / (row.vol_tot - row.vol_cop)) > 0.15 
                                        else round(row.vol_mult_other / (row.vol_tot - row.vol_cop) * 100, 1), axis=1)
    result['p_mult_cop'] = result.apply(lambda row: 
                                        labels.errors["multiples.level.high"]+str(round(row.n_mult_cop / row.n_cop * 100, 1)) 
                                        if (row.n_mult_cop / row.n_cop) > 0.15 
                                        else round(row.n_mult_cop / row.n_cop * 100, 1), axis=1)
    result['p_vol_mult_cop'] = result.apply(lambda row: 
                                        labels.errors["multiples.level.high"]+str(round(row.vol_mult_cop / row.vol_cop * 100, 1)) 
                                        if (row.vol_mult_cop / row.vol_cop) > 0.15 
                                        else round(row.vol_mult_cop / row.vol_cop * 100, 1), axis=1)

    # Clean cols
    result = result.reset_index()
    result.drop(columns=["vol_tot", "n_tot", "vol_mult", "n_mult", "vol_cop", "n_cop", "vol_mult_cop", "n_mult_cop", "vol_mult_other", "n_mult_other"], inplace=True)

    # Rename collums to match the desiered output
    result.rename(columns={ 'scan_id': 'List scan ID', 
                            'p_mult' : "% mult",#"%Ab multiples : (other+cop) / living",
                            'p_vol_mult' : "%vol mult",#"%Bv multiples (other+cop) / living",
                            'p_mult_non_cop' : "% mult (non cop)",#"%Ab multiples other / (living - cop)",
                            'p_vol_mult_non_cop' : "%vol mult (non cop)",#"%Bv multiples other / (living - cop)",
                            'p_mult_cop' : "% mult (cop)", #"%Ab multiples cop / (living - other)",
                            'p_vol_mult_cop' : "%vol mult (cop)",#"%Bv multiples cop / (living - other)"
                        }, inplace=True)

    logging.info("-- TIME : {} seconds -- : {} : {} : callback check_motoda_check".format((time.time() - start_time), _id, _mode))
    return result

"""
    
    # remove not relevant categories
  d <- filter(d,
    # not living
    ! stringr::str_detect(lineage, "^not-living"),
    # morpho stuff
    ! name %in% c("head", "part", "dead"),
    # not plankton
    ! name %in% c("Insecta", "wing", "seaweed")
  )
  
  # compute scan id and biovolume
  d$vol <- 4/3 * pi * sqrt(d$area / pi)
  d$scan_id <- str_replace(d$id, "_1_[0-9]+$", "")
  d <- group_by(d, scan_id)
  
  # compute statistics
  s <- summarise(d, n_tot=n(), vol_tot=sum(vol)) %>% 
    full_join(summarise(filter(d, stringr::str_detect(name, "multiple")), n_mult=n(), vol_mult=sum(vol)), by="scan_id") %>% 
    full_join(summarise(filter(d, stringr::str_detect(lineage, "Copepoda")), n_cop=n(), vol_cop=sum(vol)), by="scan_id") %>%
    full_join(summarise(filter(d, stringr::str_detect(lineage, "Copepoda>multiple")), n_mult_cop=n(), vol_mult_cop=sum(vol)), by="scan_id") %>% 
    full_join(summarise(filter(d, stringr::str_detect(lineage, "other>multiple")), n_mult_other=n(), vol_mult_other=sum(vol)), by="scan_id")
  # missing means not counted = 0
  s[is.na(s)] <- 0
  
  # compute percent multiples
  stats <- transmute(s,
    scan_id = scan_id,
    `% mult` = n_mult / n_tot * 100,
    `%vol mult` = vol_mult / vol_tot * 100,
    `% mult (non cop)` = n_mult_other / (n_tot - n_cop) * 100,
    `%vol mult (non cop)` = vol_mult_other / (vol_tot - vol_cop) * 100,
    `% mult (cop)` = n_mult_cop / n_cop * 100,
    `%vol mult (cop)` = vol_mult_cop / vol_cop * 100
  )
  # implement checks
  ok <- apply(stats[,-1], 1, function(x) {
    all(na.omit(x <= c(20, 20, 15, 15, 15, 15)))
  })
  # TODO make this into a boolean matrix and use it in the formating of the HTML table below
  
  return(
    list(
      test=all(ok),
      message=html_table(stats[!ok,], class="table table-condensed", digits=1, classes=rlang::quos(
        `% mult`=format_condition(`% mult`>20, "danger"),
        `%vol mult`=format_condition(`%vol mult`>20, "danger"),
        `% mult (non cop)`=format_condition(`% mult (non cop)`>15, "danger"),
        `%vol mult (non cop)`=format_condition(`%vol mult (non cop)`>15, "danger"),
        `% mult (cop)`=format_condition(`% mult (cop)`>15, "danger"),
        `%vol mult (cop)`=format_condition(`%vol mult (cop)`>15, "danger")
      ))
    )
  )
  
    """